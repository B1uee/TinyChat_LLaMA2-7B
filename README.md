# TinyChat: Optimizing LLM on Edge Devices

My solution of lab5 for [efficientml.ai course](https://efficientml.ai/).

[Tutorial document](https://docs.google.com/document/d/13IaTfPKjp0KiSBEhPdX9IxgXMIAZfiFjor37OWQJhMM/edit?usp=sharing) is here.

The implementations needed to be completed in this lab include:
1. Loop Unrolling âœ”
2. Multithreading
3. SIMD Programming
4. Multithreading with Loop Unrolling
5. Combination of All Techniques

## Related Projects

[TinyChatEngine](https://github.com/mit-han-lab/TinyChatEngine).

[TinyEngine](https://github.com/mit-han-lab/tinyengine).

[Smoothquant](https://github.com/mit-han-lab/smoothquant).

[AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq)

## Acknowledgement

[llama.cpp](https://github.com/ggerganov/llama.cpp)

[transformers](https://github.com/huggingface/transformers)
